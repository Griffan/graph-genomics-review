\section{Applications of pangenomic models}

\subsection{Error correction}

De Bruijn graph based methods have been used for correcting errors in reads.
The principle used by these methods is to build an assembly from accurate short reads and use that to correct the reads.
Assembly methods used include de Bruijn graphs \cite{Salmela2014LoRDEC, Salmela2016LORMA}, de Bruijn graphs with various cleaning operations \cite{Miclotte2016Jabba, Limasset_2019, Rautiainen_2019b, Marchet_2019}, FM-index representing all de Bruijn graphs \cite{Wang2018FMLRC} and variable order de Bruijn graphs \cite{Morisse2018HGcolor, Morisse2019Consent}.
The reads are aligned to the assembly and the path in the graph is extracted as the corrected read.
Comparisons of error correction methods \cite{Fu2019ErrorCorrectionSurvey, Zhang2019ErrorCorrectionSurvey} have found de Bruijn graph based methods to outperform read alignment based methods.

\subsection{Variant calling and genotyping}

\todo[inline]{EG: this section must be reduced dramatically}

In the era of resequencing, variant calling emerged as an essential methodology for genome inference in many species.
The advent of pangenomics represents an opportunity to improve this methodology in multiple ways, all related to the theme of leveraging prior data in the form of known variation.
These strategies can be coarsely divided into two umbrella approaches.
First, known variation can inform inference by providing a statistical prior.
Second, known variation can help applications avoid algorithmic artifacts that frustrate inference.
Together, these advantages are helping pangenomics expand the limits of what variation current technology can detect and how accurately it can do so.

In many ways, the spiritual predecessors of pangenomic variant calling are multi-sample calling methods. 
Many of these were developed alongside major population sequencing projects like the 1000 Genomes Project \cite{1000_2015}.
They co-call variants on a group of samples in a single, shared inference process \cite{Li_2011,Garrison_2012}.
The motivation for doing so mirrors the motivation for pangenomic variant calling: increased statistical power via a shared prior and reduced algorithmic bias.
The main difference today is the abundance of data available.
It is no longer necessary, or even advisable, to restrict inference to the samples obtained in one project.
Increasingly large databases of variation are publicly available to guide future analyses.
The benefits of multi-sample calling can now be brought to bear even on individual samples.

In pangenomics, it is useful distinguish between \emph{variant calling} and \emph{genotyping}.
Variant calling consists of detecting variation that is yet unobserved, whereas genotyping involves determining whether a previously observed variant is present in a new sample.
Intuitively, pangenomic methods should provide greater benefit to genotyping, and indeed most existing tools emphasize it over variant calling.
%TODO: i thought i wanted another sentence here, but i can't remember it...
% - something about the variants where mapping is the most difficult?

Another trend in genome graph methods are path-based formulations for inference.
In a genome graph, haplotypes can be expressed as paths through the graph.
Accordingly, many published tools have naturally focused on using paths to define alleles and haplotypes \cite{dilthey2015improved, sibbesen2018accurate,lee2018kourami,hickey2019genotyping,dolzhenko2019expansionhunter}.
The problem of genotyping then can be recast as finding the paths that are best supported by reads.
For variant calling, some methods augment the genome graph with novel variants so that the entire haplotype exists as a path in the graph \cite{sibbesen2018accurate,lee2018kourami,hickey2019genotyping}.
A similar approach is to infer a pangenomic reference model from read alignments to a pangenome and to then use this reference model for downstream analysis \cite{Maciuca_2016,Valenzuela_2018}, although this limits the genome model being haploid and acyclic.

% Glenn
%Genome graphs can leverage known sequence variation, available in increasingly large public databases, to improve variant identification in new samples.
%In general, variants are determined by finding paths through the graph that are best supported by mapped reads, and noting how the reads differ form the paths that best represent them.
%Projecting these paths back to the reference yields the variants, typically in VCF format.
%When only paths from graph are considered, this process is considered \emph{genotyping}.
%Novel variants can be \emph{called} by augmenting the paths with edits from the reads.
\subsubsection{Small variants}

In many ways, small variants stand to benefit the least from pangenomic variant calling and genotyping.
NGS read lengths are sufficient to span the entirety of small variants, and the associated variant calling algorithms are quite mature.
However, mapping errors from reference bias can be a source of variant calling error. 
Pangenome models provide an opportunity to mitigate these biases.

One strategy for genotyping small variants consists of using graphs of known variation in local regions to realign reads that were mapped to a linear reference.
This approach was pioneered in the 1000 Genomes Project by the GLIA software.
Acknowledging the difficulty of aligning through indels and complex regions, they developed GLIA to realign reads around such variants \cite{1000_2015}.
More recently, Graphtyper expanded the scale of this approach and incorporated pangenomic data sets.
It produces results that are competitive with state-of-the-art conventional pipelines with significantly better computational efficiency.
Further, information on known variation modestly improves Graphtyper's accuracy \cite{eggertsson2017graphtyper}.

Seven Bridges' Graph Genome Pipeline includes a variant calling module that itself is minimally pangenomic beyond its use of graph-based read mappings.
Rather, it is broadly similar to existing local assembly methods based on the linear reference \cite{Poplin_2017, Rimmer_2014} except in that it prioritizes haplotypes that are found in a genome graph \cite{Rakocevic_2019}.
%This can be seen as an example of a statistical prior on variation expressed in the pangenome; simply knowing where one should look for variants is informative.
The Graph Genome Pipeline shows advantages over conventional pipelines in variant recall, especially for indels.
Interestingly, the authors' analysis suggests that the graph-based read mappings contribute a minority of the effect.
It seems simply knowing what alleles to look for is significantly informative.

%GraphTyper \cite{eggertsson2017graphtyper} is a popular genome-graph-based caller.
%It begins with a pangenome graph created from variants in dbSNP, and iteratively updates it with variants discovered in input reads.
%Its authors showed it to be more accurate on benchmarks than top linear reference-based approaches such as GATK.
%It is also practical to run on large datasets; its authors applied it to WGS data from 28,075 samples from an Icelandic population study.
%
%A more recent approach, Graph Genome Pipeline, \cite{Rakocevic_2019}, uses a genome graph derived from the 1000 Genomes Project to improve calling accuracy.
%It has been shown to be able to find variants absent from common benchmarks such as GIAB, raising the question of whether these datasets, commonly used as truth sets in the evaluation of new methods, should be updated.

\subsubsection{Challenging genomic regions}

Certain genomic regions give disproportionate trouble to conventional variant calling methods. 
The primary cause of this trouble is high genomic diversity, which can make mapping and alignment difficult. 
Some pangenomic genotyping tools have achieved efficiency by targeting otherwise costly algorithms specifically to these regions.

Much of this research has focused on the human major histocompatibility complex (MHC) region on chromosome 6, which contains the human leukocyte antigen (HLA) genes. 
These genes are critical components of the immune system, which produces a strong selective pressure for diversity and many dramatically distinct alleles.
Determining an individual's HLA alleles, or HLA typing, is often of great medical importance.

%TODO: must cut down coverage of PRG, but i want to see how erik handles it...
Several tools have used genome graphs to enable HLA typing with NGS data.
The first such algorithm was PRG \cite{dilthey2015improved}, which selects of pair of paths through a graph constructed from a multiple sequence alignment of HLA alleles.
The paths can then be used as linear references for standard mapping and calling tools.
Later iterations of the algorithm, HLA*PRG \cite{dilthey2018hla} and HLA*LA \cite{dilthey2019hla}, began aligning reads directly to the graph.
This brought accuracy comparable to Sanger methods.
The cost of significant computation time, although HLA*LA improved the efficiency.
Kourami \cite{lee2018kourami} is a similar alignment-based method.
It achieves faster speeds than HLA*LA and comparable accuracy.
Kourami also performs some novel variant calling based on local assembly.

General purpose genotyping tools have also targeted the HLA region.
On a population of Icelandic samples, Graphtyper \cite{eggertsson2017graphtyper} produced high accuracy genotypes in minutes per sample.
However, these genotypes were at a lower level of sequence resolution than HLA*LA or Kourami.
HISAT-genotype \cite{Kim_2019} is another general purpose calling algorithm whose authors demonstrated on HLA genotyping.
Like Kourami, the algorithm is based on a graph-guided local assembly of reads, although HISAT-genotype aligns the reads with HISAT2 rather than a linear mapper.
In a sample of 17 well-characterized genomes, HISAT-genotype produced full-resolution HLA genotypes with no errors.
A larger experiment based on simulated data suggested that Kourami is more accurate than HLA-genotype on previously observed haplotypes, but less accurate if there is novel variation.


%Very diverse genomic regions (whose diversity makes them difficult to study) and low complexity regions (which can have condensed graph representations) intuitively stand to see the most improvement when moving from a linear reference to a genome graph.
%To this end, much work has been done exploring the application of graphs to the major histocompatibility complex (MHC) region on chromosome 6, which contains the human leukocyte antigen (HLA) genes.
%These genes are critical components of the immune system, which produces a strong selective pressure for diversity and many dramatically distinct alleles.
%Determining the HLA alleles in a sample, or HLA typing, is often of great importance, but due to high polymorphism within the MHC region, short read sequencing against a linear reference is not effective, and more costly Sanger sequencing is required.

%TODO: add HISAT-genotype

%As a potentially viable short-read-based approach, \citeauthor{dilthey2015improved} developed the Population Reference Graph (PRG) method.
%The PRG is created from a multiple sequence alignment (MSA) of known haplotypes and variants spanning the MHC region.
%Reads are mapped to to the graph, then used to extract a pair of ``chromotype'' paths that best represent them, which are then used linear references for standard mapping and calling tools.
%An extension, PRG*HLA \cite{dilthey2018hla}, explicitly models the HLA genes, rather than the whole MHC region.
%In this method, HLA types are imputed directly from the graph, rather than intermediate chromotypes, providing faster performance.
%A further extension, HLA*LA \cite{dilthey2019hla}, further improves runtime and adds support for long read and assembly inputs.
%Kourami \cite{lee2018kourami} is another method for HLA typing that assembles haplotypes using a graph genome as a guide, and is able to incorporate novel variation from the reads.
%HLA typing with these methods using whole genome short read data has been shown to be as accurate as the conventional Sanger sequencing based approach.

Outside of the MHC, another type of challenging variation are short tandem repeats (STRs).
STR regions also have high polymorphism due to high mutation rates.
In addition, the low complexity sequences tend to make alignment and assembly challenging.
ExpansionHunter \cite{dolzhenko2019expansionhunter} provides an approach to genotyping STRs by aligning reads to cyclic graphs that model variants' repeat structure.
The authors show that this approach can genotype STRs using short read data more accurately than standard variant calling pipelines.
HISAT-genotype \cite{Kim_2019} was demonstrated on STRs as well. 
For a single well-characterized pedigree, the authors argue that HISAT-genotype's results were more accurate than wet lab methods at genotyping the Federal Bureau of Investigation's CODIS loci.

\subsubsection{Genotyping structural variation}

The study of structural variants (SVs)---typically defined as variants affecting at least 50 bp---has also benefited from genome graphs.
SVs are difficult to call with NGS reads because they are large relative to the read length.
Long read sequencing does not share this difficulty, but this technology remains prohibitively expensive for population-scale studies or routine use.
This represents an opportunity for pangenomic methods, which can represent SVs naturally represented in genome graphs.

Bayestyper \cite{sibbesen2018accurate} compares the distribution of $k$-mers from sequencing reads to the distribution of $k$-mers along paths in the graph.
It calls structural variants with high accuracy almost irrespective of the size of the variant.
However, it has a high memory footprint, and later analysis has also suggested that its reliance on long exact matches makes it fragile to breakpoint uncertainty \cite{hickey2019genotyping}.

Paragraph \cite{chen2019paragraph} is a SV genotyper with strategy that is similar to Graphtyper's \cite{eggertsson2017graphtyper}.
For each variant, Paragraph constructs a graph of the affected genomic region and realigns reads that are nearby, as determined by mapping to a linear reference.
It then computes a genotype from the support of each allele's breakpoints in the graph alignment.
Paragraph was shown to outperform similar methods that rely on linear references by a wide margin.
\todo{EG: we need to cite the update to GraphTyper here, where it was shown to work on SVs.\\ JME: is there a citation for this? I can't find anything}


VG's SV genotyper can be run on genome graphs of arbitrary shape \cite{hickey2019genotyping}.
Unlike Paragraph, VG operates on alignments that were produced against against the graph without any intermediate use of a linear reference.
It uses the snarl decomposition \cite{paten2018superbubbles} to identify sites of variation in the graph, and derives haplotypes using read support.
It was shown to be much more accurate than linear reference-based approaches.

\subsection{Assembly}
% Erik

\subsubsection{Phasing bubble chains in single individuals or trios}

Determining the two genome sequences of diploid organisms per chromosome is important in order to correctly understand allele-specific expression and compound heterozygosity, and in order to carry out many analyses in the genetics of common diseases and in population genetics \cite{tewhey2011importance}. 
However, current assembly approaches often generate mixed haplotype assembly and, therefore, fail to capture the diploid nature of the organism under study. 

WHdenovo \cite{garg2019trio, garg2018graph} is the first tool that leverages information of short and long-reads to generate happlotype-resolved assembly. 
The basic idea is to work in the space of assembly graph (without collapses halotypes) to perform phasing. 
This method was applied to genomes of moderate sizes with low to high heterozygosity rates. 
Furthermore, we have shown that we can detect and phase structural variants.

\subsection{Transcription factor binding}
% Glenn

CHiP-seq data, reads that bind to specific transcription factors, can be mapped back to the reference genome in order to locate binding sites.
 
Graph Peak Caller is based on \texttt{vg} and is the first tool to use a genome graph for this process \cite{Grytten_2019}.
It was shown to find binding sites more enriched for known DNA binding motifs than linear methods on \emph{A. thaliana}.
It was also applied to human data to discover novel sites for enhancers in the human genome \cite{groza2019personalized}. 

\todo[inline]{Do we need a real epigenomics section about DNA base or histone modification?}

\subsection{Transcriptomics}

Some transcriptomic analyses are strongly affected by reference bias.
Chief among these is allele-specific expression (ASE) \cite{Degner2009-vw,Castel2015-ef}.
ASE analysis estimates the expression levels of genes or transcripts on each allele separately by comparing the number of RNA sequencing (RNA-seq) reads mapped to the two different alleles of heterozygous variants.
A mapping bias in favor of one of the alleles can therefore create illusory differences in expression between the alleles.

The simplest approach to using variation data in mapping involves creating a personalized diploid genome or transcriptome, which is then used as the reference for a standard linear mapping method \cite{Turro2011-op,Rozowsky_2011,Bray_2016,Raghupathy2018-sd}.
Methods using this approach have been shown to reduce reference bias and improve estimation of ASE, relative to traditional single reference genome methods, but they are limited by requiring phased haplotypes for the individuals under study.
Variant-aware mappers remove this necessity.
For example, ASElux \cite{Miao2018-ps} and GSNAP \cite{Castel2015-ef} were both shown to reduce reference bias in ASE quantification.
ASElux is significantly faster than GSNAP, but GSNAP reduces reference bias slightly more \cite{Miao2018-ps}.

%GSNAP was the first variant- and splicing-aware mapping method developed for RNA-seq data \cite{Wu2010-hv}\todo{JAS: We might want to add GSNAP introduction to the model section since it is general and also works for WGS}.
%It uses a kmer-based approach where both the genome and a set of SNVs are indexed using hash tables.
%The current version uses a suffix array in addition to the hash table.
%GSNAP is still competitive with regards to mapping accuracy, but it is generally much slower than contemporary mapping methods.
%Although not demonstrated in the initial publication, GSNAP does reduce reference bias around SNVs \cite{Castel2015-ef}.
%
%Another variant-aware method, ASElux, uses all heterozygous exonic SNVs in an individual to create a suffix array index of the alleles and their flanking sequences \cite{Miao2018-ps}. 
%This index is used to filter read pairs that does not overlap any SNVs with up to 2 mismatches. 
%The much smaller set of read pairs that pass this filter are then aligned to a different suffix array index consisting of exonic and intronic regions and pairs that align unique to a single gene are used for allele counting. 
%ASElux achieves higher accuracy for ASE estimation compared to pipelines based on linear mappers and is significantly faster than GSNAP which reduces reference bias slightly more.

%Similarly to ASElux, iMapSplice creates an index of SNV alleles and their flanking sequences \cite{Liu_2018}.
%The sequences are indexed using enhanced suffix arrays and reads are mapped to both this index and the reference genome simultaneously.
%The authors demonstrated that iMapSplice achieves higher mapping accuracy and lower reference bias compared to both a linear mapping method and HISAT2, another variant-aware method.

%GSNAP, ASElux and iMapSplice only support SNVs and are therefore not able to reduce reference bias around indels.
%HISAT2 was the first genome graph based, non-SNV-variant-aware method for splicing-aware mapping of RNA-seq data \cite{Kim_2019}. 
%This method can use SNVs, deletions of any length, and insertions up to 20~bp.
%HISAT2 uses a combination of a hierarchical graph FM index and a repeat region index to map the sequencing reads to the genome graph (see Variation Graph Mapper section for more details).
%Their benchmark only shows results for DNA sequencing data, but \citeauthor{Liu_2018} did show in their iMapSplice benchmark a reduction in reference bias for HISAT2 around SNVs using an older version.  
%
%Recently, the ability to create spliced variation graphs was added to the \texttt{vg} toolkit \cite{Garrison_2018}. 
%In these variation graphs, known splice junctions are added as edges, similar to the addition of a deletion event, while transcripts are embedded as paths through the graph. 
%This allows for existing algorithms in \texttt{vg} to also be used for variant-aware mapping of RNA-seq data. 
%\texttt{vg} supports any type of variation, but its splicing-awareness is limited to only known splice junctions, and it does not support generating supplementary alignments.
%Thus, reads that span a novel splice junction will only map partially.

Variation-aware analysis of RNA-seq data is important accurately analyzing highly polymorphic regions.
As with variant calling, the case that has generated the most interest is the HLA genes in the human MHC. 
AltHapAlignR and HLApers have both demonstrated improvements in the estimation of HLA gene/transcript expression as a result of comparing the reads against a collection of known HLA haplotypes, instead of using the linear reference \cite{Lee_2018,Aguiar2019-fy}.

Even intronic variation can be helpful for analyzing RNA-seq data. 
Intronic variation can disrupt existing splice-site motifs or create new ones, resulting in intron retention or novel splicing, respectively. 
Since the presence or absence of canonical splice-site motifs is often used as a factor when scoring alignments across splice junctions, mappers which are aware of these changes to the motifs are able to give more accurate mapping results for reads that overlaps the modified splice-junctions. 
Indeed, by using a personalized genome approach, \citeauthor{Stein_2015} were able to identify 506 personal splice junctions in 75 individuals, of which 437 were novel \cite{Stein_2015}.
Even more novel junctions were later found in the same individuals by \citeauthor{Liu_2018} using iMapSplice \cite{Liu_2018}.

\subsection{Metagenomics and quasispecies}

Most of the methods mentioned in this review has focused on the analyses of human data, however, the benefits of using graphs as a non-linear reference structure have also been demonstrated for other species.
This is especially true for bacterial metagenomics and viral quasispecies where the sequencing data consists of a mixture of closely related sequences (e.g. strains).
A graph with sequence paths here provides a natural representation of the data and methods have been developed that specifically take advantage of this. 

Mykrobe predictor uses reference de Bruijn graphs to estimate antibody resistance from \textit{S. aureus} and \textit{M. Tuberculosis} sequencing data \cite{Bradley2015-kl}.
The graphs are constructed from known resistant relevant alleles and genes and compared to a whole-genome de Bruijn graph built from the sample's sequencing reads.
The frequency of alleles and genes are predicted using read coverages on the intersecting graphs.
Using this approach the authors demonstrated accurate resistance prediction within minutes on a laptop.  

GROOT \cite{Rowe2018-bg}

Virus-VG uses variation graphs to estimate virus haplotypes within a viral quasispecies.
Virus-VG first constructs a variation graph from assembled contigs to which the sequencing reads are mapped using the vg toolkit.
Maximal-length candidate haplotype paths are then enumerated by merging overlapping contigs in a breadth-first search type procedure and their abundances estimated using an optimization algorithm that minimize the difference between predicted abundances and the mapped read coverage. 
Baaijens \textit{et al.} demonstrated more accurate abundance estimates and longer haplotypes compared to reference-based approaches.
Later, Baaijens \textit{et al.}, extended this method to allow it to scale to larger genomes, such as bacteria \cite{Baaijens2019-ha}.
VG-flow, similar to Virus-VG, builds a variation graph from the assembled contigs, however, here a min-cost flow optimization algorithm is used in the candidate haplotype path enumeration step.
This algorithm scales much better with contig number and genome size, and they showed that it also slightly improves accuracy for the smaller viral genomes compared to Virus-VG. 

One major metagenomics analysis pipeline involves the taxonomical classification and quantification of sequencing data or assembled contigs using a database of known sequences, such as bacterial and viral genomes. 
While most methods do not use graph structures for this analysis they do for the most part rely on a compressed representation of the reference sequences using for example a k-mer index or FM-index (see Breitwieser \textit{et al.} for review on classification methods \cite{Breitwieser2017-yp}). 
One method that uses a graph representation is MetaKallisto \cite{Schaeffer2017-fh}. 
This method is based on the algorithm originally developed for RNA-seq analysis, Kallisto, and uses a colored de Bruijn graphs to represent the database of known genomes \cite{Bray_2016}. 
Read are aligned to these graphs using pseudoalignment and abundances estimated from the aligned reads using an expectation-maximization algorithm. 
The authors demonstrated superior performance compared to state-of-the-art metagenomic abundance methods using simulated reads on a filtered set of 1027 bacterial genomes.
One area where graph based methods have been used extensively, is in metagenomic assembly, however, since this review mainly focuses on non-linear representations of reference structures we will not go into detail about them here. 
For more details on metagenomic assembly see Breitwieser \textit{et al.} review on assembly methods \cite{Breitwieser2017-yp}. 
