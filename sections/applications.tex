\section{Applications of pangenomic models}

\subsection{Error correction}

De Bruijn graph based methods have been used for correcting errors in reads.
The principle used by these methods is to build an assembly from accurate short reads and use that to correct the reads.
Assembly methods used include de Bruijn graphs \cite{Salmela2014LoRDEC, Salmela2016LORMA}, de Bruijn graphs with various cleaning operations \cite{Miclotte2016Jabba, Limasset_2019, Rautiainen_2019b, Marchet_2019}, FM-index representing all de Bruijn graphs \cite{Wang2018FMLRC} and variable order de Bruijn graphs \cite{Morisse2018HGcolor, Morisse2019Consent}.
The reads are aligned to the assembly and the path in the graph is extracted as the corrected read.
Comparisons of error correction methods \cite{Fu2019ErrorCorrectionSurvey, Zhang2019ErrorCorrectionSurvey} have found de Bruijn graph based methods to outperform read alignment based methods.

\subsection{Variant calling and genotyping}

In the era of resequencing, variant calling emerged as an essential methodology for genome inference in many species.
The advent of pangenomics represents an opportunity to improve this methodology in multiple ways, all related to the theme of leveraging prior data in the form of known variation.
These strategies can be coarsely divided into two umbrella approaches.
First, known variation can inform inference by providing a statistical prior.
Second, known variation can help applications avoid algorithmic artifacts that frustrate inference.
Together, these advantages are helping pangenomics expand the limits of what variation current technology can detect and how accurately it can do so.

In many ways, the spiritual predecessors of pangenomic variant calling are multi-sample calling methods. 
Many of these were developed alongside major population sequencing projects like the 1000 Genomes Project \cite{1000_2015}.
They co-call variants on a group of samples in a single, shared inference process \cite{Li_2011,Garrison_2012}.
The motivation for doing so mirrors the motivation for pangenomic variant calling: increased statistical power via a shared prior and reduced algorithmic bias.
The main difference today is the abundance of data available.
It is no longer necessary, or even advisable, to restrict inference to the samples obtained in one project.
Increasingly large databases of variation are publicly available to guide future analyses.
The benefits of multi-sample calling can now be brought to bear even on individual samples.

In pangenomics, it is useful distinguish between \emph{variant calling} and \emph{genotyping}.
Variant calling concerns detecting variation that is yet unobserved, whereas genotyping involves determining whether a previously observed variant is present in a new sample.
Intuitively, pangenomic methods should provide greater benefit to genotyping, and indeed most existing tools emphasize it over variant calling.
%TODO: i thought i had another sentence here, but i can't remember it...
% - something about the variants where mapping is the most difficult

Another trend in genome graph methods are path-based formulations for inference.
In a genome graph, haplotypes can be expressed as paths through the graph.
Accordingly, many published tools have naturally focused on using paths to define alleles and haplotypes \cite{dilthey2015improved, sibbesen2018accurate,lee2018kourami,hickey2019genotyping,dolzhenko2019expansionhunter}.
The problem of genotyping then can be recast as finding the paths that are best supported by reads.
For variant calling, some methods augment the genome graph with novel variants so that the entire haplotype exists as a path in the graph \cite{sibbesen2018accurate,lee2018kourami,hickey2019genotyping}.

% Glenn
%Genome graphs can leverage known sequence variation, available in increasingly large public databases, to improve variant identification in new samples.
%In general, variants are determined by finding paths through the graph that are best supported by mapped reads, and noting how the reads differ form the paths that best represent them.
%Projecting these paths back to the reference yields the variants, typically in VCF format.
%When only paths from graph are considered, this process is considered \emph{genotyping}.
%Novel variants can be \emph{called} by augmenting the paths with edits from the reads.
\subsubsection{Small variants}

In many ways, small variants stand to benefit the least from pangenomic variant calling and genotyping.
NGS read lengths are sufficient to span the entirety of small variants, and the associated variant calling algorithms are quite mature.
However, pangenomes still have benefits to offer for small variants.
First, structural variants and regions of otherwise dense variation can confound read mappers, leading to mis-mappings and missing mappings.
This can cause variant calling errors.
Second, missing variation in the reference can cause base-level alignment artifacts.
Mapping and aligning to pangenome models provides the opportunity to mitigate these biases.

One strategy for genotyping small variants consists of using graphs of known variation in local regions to realign reads that were mapped to a linear reference.
This approach was pioneered in the 1000 Genomes Project by the GLIA software.
Acknowledging the difficulty of aligning through indels and complex regions, they developed GLIA to realign reads around such variants \cite{1000_2015}.
Graphtyper later expanded the scale of this approach and incorporated pangenomic data sets.
It produces results that are competitive with state-of-the-art conventional pipelines with significantly increased efficiency.
Further, the addition of known variants modestly improves Graphtyper's accuracy \cite{eggertsson2017graphtyper}.

Seven Bridges' Graph Genome Pipeline includes a variant calling module that itself is minimally pangenomic.
Rather, it is broadly similar to existing local assembly methods based on the linear reference except in that it prioritizes haplotypes that are found in a genome graph.
This can be seen as an example of a statistical prior on variation expressed in the pangenome: simply knowing where one should look for variants is informative.
With this feature and graph-based read mappings, the Graph Genome Pipeline shows advantages over conventional pipelines in variant recall, especially for indels.
Interestingly, the authors' analysis suggests that the graph-based mappings contribute a minority of the effect.

%GraphTyper \cite{eggertsson2017graphtyper} is a popular genome-graph-based caller.
%It begins with a pangenome graph created from variants in dbSNP, and iteratively updates it with variants discovered in input reads.
%Its authors showed it to be more accurate on benchmarks than top linear reference-based approaches such as GATK.
%It is also practical to run on large datasets; its authors applied it to WGS data from 28,075 samples from an Icelandic population study.
%
%A more recent approach, Graph Genome Pipeline, \cite{Rakocevic_2019}, uses a genome graph derived from the 1000 Genomes Project to improve calling accuracy.
%It has been shown to be able to find variants absent from common benchmarks such as GIAB, raising the question of whether these datasets, commonly used as truth sets in the evaluation of new methods, should be updated.

\subsubsection{Challenging genomic regions}

Certain genomic regions give disproportionate trouble to conventional variant calling methods. 
The primary cause of this trouble is high genomic diversity, which can make mapping and alignment difficult. 
Some pangenomic genotyping methods have been developed that take advantage of the fact that these regions are relatively spatially contained. 
Accordingly, these tools can target otherwise costly algorithms specifically to these regions.

Much of this research has focused on the major histocompatibility complex (MHC) region on chromosome 6, which contains the human leukocyte antigen (HLA) genes. 
These genes are critical components of the immune system, which produces a strong selective pressure for diversity and many dramatically distinct alleles.
Determining the HLA alleles in a sample, or HLA typing, is often of great importance. 
However, due to high polymorphism within the MHC region, short read sequencing against a linear reference is not effective.
Thus, HLA typing has relied on more costly Sanger sequencing.

All genome-graph based HLA genotyping tool have focused on cheaper high throughput sequencing.
The first such algorithm was PRG \cite{dilthey2015improved}, which uses the $k$-mers from a set of reads to select of pair of paths through a graph constructed from a multiple sequence alignment of HLA alleles.
The paths can then be used as linear references for standard mapping and calling tools.
A later iteration of the algorithm, HLA*PRG \cite{dilthey2018hla}, began aligning reads directly to the graph.
This brought accuracy comparable to gold-standard methods, but at the cost of significant computation time.
A further iteration, HLA*LA \cite{dilthey2019hla}, accelerated the alignment step by first mapping to GRCh38 and then making small, local edits to the corresponding alignment in the graph.
This modification also lets it support long reads and assembly input.
Kourami \cite{lee2018kourami} is a similar method that also takes the approach of aligning reads to a graph of the HLA region.
However, it approximates full graph alignment by aligning to a large database of linear HLA haplotypes, which allows it to use faster alignment algorithms.
It achieves significantly faster speeds than even HLA*LA, and comparable accuracy on high-coverage short reads.
Kourami also performs some novel variant calling based on local assembly.

General purpose genotyping tools have also targeted the HLA region.
On a population of Icelandic samples, Graphtyper \cite{eggertsson2017graphtyper} produced genotypes with high accuracy in only minutes per sample.
However, these genotypes were at a lower level of sequence resolution than the PRG tools or Kourami.
Like Graphtyper, HISAT-genotype \cite{Kim_2019} is a general purpose calling algorithm whose authors demonstrated on HLA genotyping.
Like Kourami, the algorithm is based on a graph-guided local assembly of reads, although HISAT-genotype aligns the reads with HISAT2 rather than a linear mapper.
In a sample of 17 genomes, HISAT-genotype produced full-resolution HLA genotypes with no errors.
A larger simulation study suggested that Kourami is more accurate than HLA-genotype on previously observed haplotypes, but less accurate if there is novel variation.


%Very diverse genomic regions (whose diversity makes them difficult to study) and low complexity regions (which can have condensed graph representations) intuitively stand to see the most improvement when moving from a linear reference to a genome graph.
%To this end, much work has been done exploring the application of graphs to the major histocompatibility complex (MHC) region on chromosome 6, which contains the human leukocyte antigen (HLA) genes.
%These genes are critical components of the immune system, which produces a strong selective pressure for diversity and many dramatically distinct alleles.
%Determining the HLA alleles in a sample, or HLA typing, is often of great importance, but due to high polymorphism within the MHC region, short read sequencing against a linear reference is not effective, and more costly Sanger sequencing is required.

%TODO: add HISAT-genotype

%As a potentially viable short-read-based approach, \citeauthor{dilthey2015improved} developed the Population Reference Graph (PRG) method.
%The PRG is created from a multiple sequence alignment (MSA) of known haplotypes and variants spanning the MHC region.
%Reads are mapped to to the graph, then used to extract a pair of ``chromotype'' paths that best represent them, which are then used linear references for standard mapping and calling tools.
%An extension, PRG*HLA \cite{dilthey2018hla}, explicitly models the HLA genes, rather than the whole MHC region.
%In this method, HLA types are imputed directly from the graph, rather than intermediate chromotypes, providing faster performance.
%A further extension, HLA*LA \cite{dilthey2019hla}, further improves runtime and adds support for long read and assembly inputs.
%Kourami \cite{lee2018kourami} is another method for HLA typing that assembles haplotypes using a graph genome as a guide, and is able to incorporate novel variation from the reads.
%HLA typing with these methods using whole genome short read data has been shown to be as accurate as the conventional Sanger sequencing based approach.

Beyond the MHC, another type of challenging region on short tandem repeats (STRs).
These regions also have high polymorphism due to high mutation rates.
In addition, the low complexity sequences tend to make alignment and assembly challenging.
ExpansionHunter \cite{dolzhenko2019expansionhunter} provides an approach using genome graph models.
It uses cyclic graphs to model the repeat structure of STR variants.
The graph is then used as a target for read alignments.
The authors show that this approach can more accurately genotype clinically relevant sites of such variation using short read data compared to standard variant calling pipelines.
HISAT-genotype \cite{Kim_2019} was demonstrated on STRs as well. 
For a single well-characterized pedigree, the authors argue that HISAT-genotype's results were more accurate than wet lab methods at genotyping the Federal Bureau of Investigation's CODIS loci.

\subsubsection{Genotyping structural variation}

The study of structural variants (SVs), mutational events of at least 50~bp in length, is also benefiting from the application of genome graphs.
SVs can be increasingly well characterized by long read sequencing, but this technology remains prohibitively expensive for population scale studies.
Known SVs can be naturally represented as genome graphs, and genotyped from short read data using a selection of tools.

Bayestyper \cite{sibbesen2018accurate} uses the distribution of kmers from sequencing reads, as well as that of the graph, to accurately identify variant haplotypes.
\todo{EG: we need to cite the update to GraphTyper here, where it was shown to work on SVs.}

Paragraph \cite{chen2019paragraph} is a SV genotyper that operates on a genome graphs constructed from each variant in a VCF of SV calls.
For each variant, it remaps nearby reads as retrieved from a linear alignment, to the graph.
It then computes a genotype from the support of each allele's breakpoints in the graph alignment.
Paragraph was shown to outperform similar methods that rely on linear references by a wide margin.

\texttt{vg}'s SV genotyper can be run on any genome graph that contains embedded paths, which are required as the coordinate space for VCF output \cite{hickey2019genotyping}.
Unlike all other methods mentioned above, which remap a subset of relevant reads to the graph, \texttt{vg} operates directly on an alignment of the entire read set against the graph.
It uses the snarl decomposition \cite{paten2018superbubbles} to identify sites of variation in the graph, and derives haplotypes using read support.
It was shown to be much more accurate than linear reference-based approaches, and was also evaluated on graphs derived from alignments of assemblies.

\subsection{Assembly}
% Erik

\subsection{Transcription factor binding}
% Glenn

CHiP-seq data, reads that bind to specific transcription factors, can be mapped back to the reference genome in order to locate binding sites.
 
Graph Peak Caller is based on \texttt{vg} and is the first tool to use a genome graph for this process \cite{Grytten_2019}.
It was shown to find binding sites more enriched for known DNA binding motifs than linear methods on \emph{A.
thaliana}.
It was also applied to human data to discover novel sites for enhancers in the human genome \cite{groza2019personalized}. 

\todo[inline]{Do we need a real epigenomics section about DNA base or histone modification?}

\subsection{Transcriptomics}

Attempts to use genome graphs to analyze transcriptomic sequencing data have generally received less attention than similar experiments with whole genome sequencing data.
However, when estimating allele-specific expression (ASE) reference bias is an important issue that needs to be addressed \cite{Degner2009-vw,Castel2015-ef}.
ASE analysis estimates the expression levels of genes or transcripts on each allele separately, by simply put comparing the number of RNA sequencing (RNA-seq) reads mapped to the two different alleles of heterozygous variants.
A mapping bias in favor of one of the alleles can therefore result in a false-positive difference in expression between the alleles.
Variant-aware methods, including those based on genome graphs, allow for the sequence difference between the two alleles to be considered during mapping thus reducing reference bias.

The simplest approach to using variation data in mapping involves creating a personalized diploid genome or transcriptome, which is then used as the reference for a standard linear mapping method \cite{Turro2011-op,Rozowsky_2011,Bray_2016,Raghupathy2018-sd}.
Methods using this approach have been shown to reduce reference bias and improve estimation of ASE, relative to traditional single reference genome methods, but they are limited by requiring phased haplotypes for the individuals under study.

Proper variant-aware mapping methods, on the other hand, do not require that the variants are phased beforehand.
GSNAP was the first variant- and splicing-aware mapping method developed for RNA-seq data \cite{Wu2010-hv}\todo{JAS: We might want to add GSNAP introduction to the model section since it is general and also works for WGS}.
It uses a kmer-based approach where both the genome and a set of SNVs are indexed using hash tables.
The current version uses a suffix array in addition to the hash table.
GSNAP is still competitive with regards to mapping accuracy, but it is generally much slower than contemporary mapping methods.
Although not demonstrated in the initial publication, GSNAP does reduce reference bias around SNVs \cite{Castel2015-ef}.

Another variant-aware method, ASElux, uses all heterozygous exonic SNVs in an individual to create a suffix array index of the alleles and their flanking sequences \cite{Miao2018-ps}. 
This index is used to filter read pairs that does not overlap any SNVs with up to 2 mismatches. 
The much smaller set of read pairs that pass this filter are then aligned to a different suffix array index consisting of exonic and intronic regions and pairs that align unique to a single gene are used for allele counting. 
ASElux achieves higher accuracy for ASE estimation compared to pipelines based on linear mappers and is significantly faster than GSNAP which reduces reference bias slightly more.

Similarly to ASElux, iMapSplice creates an index of SNV alleles and their flanking sequences \cite{Liu_2018}.
The sequences are indexed using enhanced suffix arrays and reads are mapped to both this index and the reference genome simultaneously.
The authors demonstrated that iMapSplice achieves higher mapping accuracy and lower reference bias compared to both a linear mapping method and HISAT2, another variant-aware method.

GSNAP, ASElux and iMapSplice only support SNVs and are therefore not able to reduce reference bias around indels.
HISAT2 was the first genome graph based, non-SNV-variant-aware method for splicing-aware mapping of RNA-seq data \cite{Kim_2019}. 
This method can use SNVs, deletions of any length, and insertions up to 20~bp.
HISAT2 uses a combination of a hierarchical graph FM index and a repeat region index to map the sequencing reads to the genome graph (see Variation Graph Mapper section for more details).
Their benchmark only shows results for DNA sequencing data, but \citeauthor{Liu_2018} did show in their iMapSplice benchmark a reduction in reference bias for HISAT2 around SNVs using an older version.  

Recently, the ability to create spliced variation graphs was added to the \texttt{vg} toolkit \cite{Garrison_2018}. 
In these variation graphs, known splice junctions are added as edges, similar to the addition of a deletion event, while transcripts are embedded as paths through the graph. 
This allows for existing algorithms in \texttt{vg} to also be used for variant-aware mapping of RNA-seq data. 
\texttt{vg} supports any type of variation, but its splicing-awareness is limited to only known splice junctions, and it does not support generating supplementary alignments.
Thus, reads that span a novel splice junction will only map partially.

As with whole genome DNA sequencing data, variation-aware analysis of RNA-seq data is important for getting accurate results in highly polymorphic regions, such as the HLA genes in the human major histocompatibility complex region. 
AltHapAlignR and HLApers have both demonstrated improvements in the estimation of HLA gene/transcript expression as a result of comparing the reads against a collection of known HLA haplotypes, instead of using the linear reference \cite{Lee_2018,Aguiar2019-fy}.

Leveraging variation even in the intronic regions can be helpful when analysing RNA-seq data. 
Variation in these regions can disrupt existing splice-site motifs or create new ones, resulting in intron retention or novel splicing, respectively. 
Since the presence or absence of canonical splice-site motifs is often used as a factor when scoring alignments across splice junctions, mappers which are aware of these changes to the motifs are able to give more accurate mapping results for reads that overlaps the modified splice-junctions. 
Indeed, by using a personalized genome approach, \citeauthor{Stein_2015} were able to identify 506 personal splice junctions in 75 individuals, of which 437 were novel \cite{Stein_2015}.
Even more novel junctions were later found in the same individuals by \citeauthor{Liu_2018} using iMapSplice \cite{Liu_2018}.

\subsection{Metagenomics and quasispecies}

Most of the methods mentioned in this review has focused on the analyses of human data, however, the benefits of using graphs as a non-linear reference structure have also been demonstrated for other species.
This is especially true for bacterial metagenomics and viral quasispecies where the sequencing data consists of a mixture of closely related sequences (e.g. strains).
A graph with sequence paths here provides a natural representation of the data and methods have been developed that specifically take advantage of this. 

Mykrobe predictor uses reference de Bruijn graphs to estimate antibody resistance from \textit{S. aureus} and \textit{M. Tuberculosis} sequencing data \cite{Bradley2015-kl}.
The graphs are constructed from known resistant relevant alleles and genes and compared to a whole-genome de Bruijn graph built from the sample's sequencing reads.
The frequency of alleles and genes are predicted using read coverages on the intersecting graphs.
Using this approach the authors demonstrated accurate resistance prediction within minutes on a laptop.  

GROOT \cite{Rowe2018-bg}

Virus-VG uses variation graphs to estimate virus haplotypes within a viral quasispecies.
Virus-VG first constructs a variation graph from assembled contigs to which the sequencing reads are mapped using the vg toolkit.
Maximal-length candidate haplotype paths are then enumerated by merging overlapping contigs in a breadth-first search type procedure and their abundances estimated using an optimization algorithm that minimize the difference between predicted abundances and the mapped read coverage. 
Baaijens \textit{et al.} demonstrated more accurate abundance estimates and longer haplotypes compared to reference-based approaches.
Later, Baaijens \textit{et al.}, extended this method to allow it to scale to larger genomes, such as bacteria \cite{Baaijens2019-ha}.
VG-flow, similar to Virus-VG, builds a variation graph from the assembled contigs, however, here a min-cost flow optimization algorithm is used in the candidate haplotype path enumeration step.
This algorithm scales much better with contig number and genome size, and they showed that it also slightly improves accuracy for the smaller viral genomes compared to Virus-VG. 

One major metagenomics analysis pipeline involves the taxonomical classification and quantification of sequencing data or assembled contigs using a database of known sequences, such as bacterial and viral genomes. 
While most methods do not use graph structures for this analysis they do for the most part rely on a compressed representation of the reference sequences using for example a k-mer index or FM-index (see Breitwieser \textit{et al.} for review on classification methods \cite{Breitwieser2017-yp}). 
One method that uses a graph representation is MetaKallisto \cite{Schaeffer2017-fh}. 
This method is based on the algorithm originally developed for RNA-seq analysis, Kallisto, and uses a colored de Bruijn graphs to represent the database of known genomes \cite{Bray_2016}. 
Read are aligned to these graphs using pseudoalignment and abundances estimated from the aligned reads using an expectation-maximization algorithm. 
The authors demonstrated superior performance compared to state-of-the-art metagenomic abundance methods using simulated reads on a filtered set of 1027 bacterial genomes.
One area where graph based methods have been used extensively, is in metagenomic assembly, however, since this review mainly focuses on non-linear representations of reference structures we will not go into detail about them here. 
For more details on metagenomic assembly see Breitwieser \textit{et al.} review on assembly methods \cite{Breitwieser2017-yp}. 
